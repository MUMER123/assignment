{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.\n",
    "\n",
    "Logistic regression is implemented in LogisticRegression. This implementation can fit binary, One-vs-Rest, or multinomial logistic regression with optional \n",
    ", \n",
    " or Elastic-Net regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Activation Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identity:\n",
    "The identity activation function returns its input as it is.\n",
    "\n",
    "identity (a)=a\n",
    "\n",
    "It is the simplest of all activation functions but does not impart any particular characteristic to the input. It is mostly reserved for output layers, especially in the case of real-valued regression problems.\n",
    "\n",
    "\n",
    "### Sigmoid: σ\n",
    "\n",
    "The sigmoid activation, typically denoted as \n",
    "σ(a), is a nonlinear activation function with the range [0,1].\n",
    "\n",
    "σ(a)=1/1+e−a\n",
    "\n",
    "It is commonly used for gates in LSTMs and GRUs. It can also be used for probabilistic outputs because it is always positive and less than 1.\n",
    "\n",
    "It is also known as the logistic or soft-step activation function.\n",
    "\n",
    "\n",
    "The sigmoid function is an apt choice for predicting a probabilistic output. This is possible because the output of the sigmoid function is bounded in the range [0,1].\n",
    "\n",
    "The sigmoid function output is 0.5 only when its input is 0.\n",
    "\n",
    "For positive inputs, the sigmoid returns values in the range [0.5,1].\n",
    "\n",
    "For negative inputs, the sigmoid returns values in the range [0,0.5].\n",
    "\n",
    "### Hyperbolic tangent: tanh\n",
    "The hyperbolic tangent activation, typically denoted as tanh(a), is a nonlinear activation function with the range [−1,1].\n",
    "\n",
    "It is quite similar to the sigmoid activation function, but allows for negative values.\n",
    "\n",
    "tanh(a)=(e^a)−(e^−a)/(e^a)+(e^−a)=e^2a−1/e^2a+1\n",
    "\n",
    "### ReLU:\n",
    "\n",
    "Rectified linear unit (ReLU) is a piecewise linear function that assigns zero to negative input and keeps positive input unchanged. It is typically denoted as its acronym ReLU.\n",
    "\n",
    "ReLU(a)=max{0,a}\n",
    "\n",
    "ReLU is the default recommendation for all hidden layers in modern deep neural networks. Multiple stacked layers with ReLU activations enable the modeling of any nonlinearity due to the piecewise linearity of this activation function.\n",
    "\n",
    "### Leaky ReLU:\n",
    "ReLU is harsh on negative inputs. It returns zero for negative inputs. This rigidity results in dead units — units whose activation is always zero.\n",
    "\n",
    "A milder alternative is the leaky ReLU, defined as follows:\n",
    "\n",
    "ReLU(a)=⎧0.01         for a<0\n",
    "        ⎨\n",
    "        ⎩a            for a≥0\n",
    " \n",
    "Thus, negative values are reduced in magnitude, but still manage to pass through, thereby preventing dead units.\n",
    "\n",
    "### Parametric ReLU: PReLU\n",
    "\n",
    "The leaky ReLU discussed above makes an arbitrary choice of returning \n",
    "0.01a when a<0.\n",
    "\n",
    "The multiplier 0.01 can instead by parametrized with a learnable parameter α that can be adapted during learning phase, just as any parameter of the model.\n",
    "\n",
    "ReLU(a)=⎧αa for a<0\n",
    "        ⎨\n",
    "        ⎩a  for a≥0\n",
    "\n",
    "### SoftPlus\n",
    "ReLU, Leaky ReLU, and PReLU are not differentiable at zero. A softer alternative that is differentiable, but has a behaviour roughly similar to ReLU is the SoftPlus activation function.\n",
    "\n",
    "SoftPlus(a)=ln(1+e^−a)\n",
    "\n",
    "In spite of this differentiable behavior, it is still the case that ReLU is preferred and default choice in neural networks. It often works well enough in practice and is super cheap to compute."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "80c120683e98059cd44d4ab93f2cea7b1e32b20ff58d21dccc771fb8d21cfc2f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
